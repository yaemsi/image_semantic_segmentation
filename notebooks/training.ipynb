{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5ce59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from argparse import Namespace\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    PreTrainedModel, \n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        # preds: [batch, 1, H, W] - sigmoid outputs (0 to 1)\n",
    "        # targets: [batch, 1, H, W] - binary masks (0 or 1)\n",
    "        \n",
    "        # Flatten to 1D for easier computation\n",
    "        preds_flat = preds.view(preds.size(0), -1)\n",
    "        targets_flat = targets.view(targets.size(0), -1)\n",
    "        \n",
    "        intersection = (preds_flat * targets_flat).sum(dim=1)\n",
    "        union = preds_flat.sum(dim=1) + targets_flat.sum(dim=1)\n",
    "        \n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()  # Average over batch\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, smooth=1e-6):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha  # Weight for Dice (1 - alpha for BCE)\n",
    "        self.dice = DiceLoss(smooth=smooth)\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        dice_loss = self.dice(preds, targets)\n",
    "        bce_loss = self.bce(preds, targets)\n",
    "        return self.alpha * dice_loss + (1 - self.alpha) * bce_loss\n",
    "\n",
    "def seg_data_collator(features):\n",
    "    pixel_values = torch.stack([f[\"pixel_values\"] for f in features])\n",
    "    labels = torch.stack([f[\"label\"] for f in features])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the transformations\n",
    "train_transform = A.Compose([\n",
    "    # 1. Geometric: Handles both image and mask\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Affine(\n",
    "        translate_percent=(-0.0625, 0.0625), # Roughly maps to shift_limit\n",
    "        scale=(-0.9, 1.1),              # 1 - scale_limit to 1 + scale_limit\n",
    "        rotate=(-15, 15),              # rotate_limit\n",
    "        p=0.5\n",
    "    ),\n",
    "    \n",
    "    # 2. Photometric: Only affects the image\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.4),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    \n",
    "    # 3. Robustness: CoarseDropout to force feature learning\n",
    "    A.CoarseDropout(\n",
    "        num_holes_range=(2, 4), \n",
    "        hole_height_range=(10, 20), \n",
    "        hole_width_range=(10, 20), \n",
    "        #num_holes_range=(3, 6),\n",
    "        #hole_height_range=(10, 20),\n",
    "        #hole_width_range=(10, 20),\n",
    "        p=0.3),\n",
    "    \n",
    "    # 4. Normalization (using ImageNet stats)\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "\n",
    "def preprocess_fn(examples):\n",
    "    images = [np.array(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    # Ensure masks are single-channel (L) for segmentation\n",
    "    masks = [np.array(mask.convert(\"L\")) for mask in examples[\"label\"]]\n",
    "    \n",
    "    inputs = {\"pixel_values\": [], \"labels\": []}\n",
    "    \n",
    "    for img, mask in zip(images, masks):\n",
    "        # Apply Albumentations\n",
    "        augmented = train_transform(image=img, mask=mask)\n",
    "        \n",
    "        inputs[\"pixel_values\"].append(augmented[\"image\"])\n",
    "        # Ensure mask is long type and scaled (0 and 1)\n",
    "        inputs[\"labels\"].append(augmented[\"label\"].long())\n",
    "        \n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d387f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "    # Upsample logits to match label size\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(\n",
    "        predictions=predictions, \n",
    "        references=labels, \n",
    "        num_labels=2, \n",
    "        ignore_index=255\n",
    "    )\n",
    "\n",
    "class LogoSegmentationTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")           # shape (batch, h, w)\n",
    "        outputs = model(**inputs)               # Segformer outputs\n",
    "        logits = outputs.logits                 # (batch, 2, h, w)\n",
    "\n",
    "        # Option A: Keep CE (very stable baseline)\n",
    "        # loss_fct = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
    "        # loss = loss_fct(logits, labels.long())\n",
    "\n",
    "        # Option B: Dice + BCE (your preference)\n",
    "        preds = torch.softmax(logits, dim=1)[:, 1]   # prob of logo class\n",
    "        preds = preds.unsqueeze(1)                   # (B,1,H,W)\n",
    "\n",
    "        labels_binary = (labels == 1).float().unsqueeze(1)  # (B,1,H,W)\n",
    "\n",
    "        # Reuse your earlier losses\n",
    "        #dice_loss = DiceLoss()(preds, labels_binary)\n",
    "        #dice_loss = nn.BCELoss()(preds, labels_binary)\n",
    "        \n",
    "        #bce_loss  = F.binary_cross_entropy_with_logits(logits[:,1,:,:], labels_binary.squeeze(1))\n",
    "        # or F.binary_cross_entropy(preds, labels_binary)\n",
    "\n",
    "        #loss = 0.5 * dice_loss + 0.5 * bce_loss\n",
    "        loss = CombinedLoss()(preds, labels_binary)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c33ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kwargs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mmodel = SegformerForSemanticSegmentation.from_pretrained(\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    \"nvidia/segformer-b0-finetuned-ade-512-512\",\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \u001b[33;03m)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mUNetPlusPlusConfig\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mPretrainedConfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munetplusplus\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mresnet34\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43maux_params\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mUNetPlusPlusConfig\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mUNetPlusPlusConfig\u001b[39;00m(PretrainedConfig):\n\u001b[32m      9\u001b[39m     model_type = \u001b[33m\"\u001b[39m\u001b[33munetplusplus\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     11\u001b[39m         \u001b[38;5;28mself\u001b[39m, \n\u001b[32m     12\u001b[39m         encoder_name=\u001b[33m'\u001b[39m\u001b[33mresnet34\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     13\u001b[39m         encoder_depth=\u001b[32m5\u001b[39m, \n\u001b[32m     14\u001b[39m         encoder_weights=\u001b[33m'\u001b[39m\u001b[33mimagenet\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     15\u001b[39m         decoder_use_norm=\u001b[33m'\u001b[39m\u001b[33mbatchnorm\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     16\u001b[39m         decoder_channels=(\u001b[32m256\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m32\u001b[39m, \u001b[32m16\u001b[39m), \n\u001b[32m     17\u001b[39m         decoder_attention_type=\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[32m     18\u001b[39m         decoder_interpolation=\u001b[33m'\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     19\u001b[39m         in_channels=\u001b[32m3\u001b[39m, \n\u001b[32m     20\u001b[39m         classes=\u001b[32m1\u001b[39m, \n\u001b[32m     21\u001b[39m         activation=\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[32m     22\u001b[39m         aux_params=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m         **\u001b[43mkwargs\u001b[49m):\n\u001b[32m     25\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(**kwargs)\n\u001b[32m     26\u001b[39m         \u001b[38;5;28mself\u001b[39m.encoder_name = encoder_name\n",
      "\u001b[31mNameError\u001b[39m: name 'kwargs' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
    "    num_labels=1,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\"\"\"\n",
    "class UNetPlusPlusConfig(PretrainedConfig):\n",
    "    model_type = \"unetplusplus\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder_name='resnet34', \n",
    "        encoder_depth=5, \n",
    "        encoder_weights='imagenet', \n",
    "        decoder_use_norm='batchnorm', \n",
    "        decoder_channels=(256, 128, 64, 32, 16), \n",
    "        decoder_attention_type=None, \n",
    "        decoder_interpolation='nearest', \n",
    "        in_channels=3, \n",
    "        classes=1, \n",
    "        activation=None, \n",
    "        aux_params=None,\n",
    "        **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder_name = encoder_name\n",
    "        self.encoder_depth = encoder_depth\n",
    "        self.encoder_weights = encoder_weights\n",
    "        self.decoder_use_norm = decoder_use_norm\n",
    "        self.decoder_channels = decoder_channels\n",
    "        self.decoder_attention_type = decoder_attention_type\n",
    "        self.decoder_interpolation = decoder_interpolation\n",
    "        self.in_channels = in_channels\n",
    "        self.classes = classes\n",
    "        self.activation = activation\n",
    "        self.aux_params = aux_params\n",
    "\n",
    "class UNetPlusPlusHF(PreTrainedModel):\n",
    "    config_class = UNetPlusPlusConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = smp.UnetPlusPlus(\n",
    "            encoder_name=config.encoder_name,\n",
    "            encoder_depth=config.encoder_depth, \n",
    "            encoder_weights=config.encoder_weights, \n",
    "            decoder_use_norm=config.decoder_use_norm, \n",
    "            decoder_channels=config.decoder_channels, \n",
    "            decoder_attention_type=config.decoder_attention_type, \n",
    "            decoder_interpolation=config.decoder_interpolation, \n",
    "            in_channels=config.in_channels, \n",
    "            classes=config.classes, \n",
    "            activation=config.activation, \n",
    "            aux_params=config.aux_params\n",
    "        )\n",
    "    \"\"\"\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        logits = self.model(pixel_values)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # We reuse the DiceBCELoss logic from before\n",
    "            loss_fct = CombinedLoss()\n",
    "            # SMP outputs [Batch, Classes, H, W]\n",
    "            # We take the 'logo' channel (index 1) for binary comparison\n",
    "            loss = loss_fct(logits[:, 1, :, :], labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "    \"\"\"\n",
    "\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name='resnet34', \n",
    "    encoder_depth=5, \n",
    "    encoder_weights='imagenet', \n",
    "    decoder_use_norm='batchnorm', \n",
    "    decoder_channels=(256, 128, 64, 32, 16), \n",
    "    decoder_attention_type=None, \n",
    "    decoder_interpolation='nearest', \n",
    "    in_channels=3, \n",
    "    classes=1, \n",
    "    activation=None, \n",
    "    aux_params=None\n",
    "    )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output_image_segmentation/\",\n",
    "    learning_rate=6e-5,\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,    # adjust to your GPU (4â€“16 typical)\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,       # important for custom datasets\n",
    "    push_to_hub=False,                 # set True later if you want\n",
    "    report_to=\"none\",                  # or \"wandb\", \"tensorboard\"\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mean_iou\",\n",
    "    greater_is_better=True,\n",
    "    bf16=True,                         # if GPU supports\n",
    "    save_total_limit = 3,\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    do_predict = True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e30cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = f\"/home/yassir/projects/image_semantic_segmentation/data/processed/\"\n",
    "ds = load_from_disk(ds_path)\n",
    "\n",
    "# Apply to your dataset\n",
    "ds[\"train\"].set_transform(preprocess_fn)\n",
    "ds[\"validation\"].set_transform(preprocess_fn)\n",
    "ds[\"test\"].set_transform(preprocess_fn)\n",
    "\n",
    "\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name='resnet34', \n",
    "    encoder_depth=5, \n",
    "    encoder_weights='imagenet', \n",
    "    decoder_use_norm='batchnorm', \n",
    "    decoder_channels=(256, 128, 64, 32, 16), \n",
    "    decoder_attention_type=None, \n",
    "    decoder_interpolation='nearest', \n",
    "    in_channels=3, \n",
    "    classes=1, \n",
    "    activation=None, \n",
    "    aux_params=None,\n",
    "    )\n",
    "model.config = Namespace(**model.config)\n",
    "model.config.use_cache = True\n",
    "\n",
    "trainer = LogoSegmentationTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=seg_data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = f\"/home/yassir/projects/image_semantic_segmentation/data/processed/\"\n",
    "ds = load_from_disk(ds_path)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5bb2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_semantic_segmentation",
   "language": "python",
   "name": "image_semantic_segmentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
