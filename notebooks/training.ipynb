{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5ce59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from argparse import Namespace\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    PreTrainedModel, \n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        # preds: [batch, 1, H, W] - sigmoid outputs (0 to 1)\n",
    "        # targets: [batch, 1, H, W] - binary masks (0 or 1)\n",
    "        \n",
    "        # Flatten to 1D for easier computation\n",
    "        preds_flat = preds.view(preds.size(0), -1)\n",
    "        targets_flat = targets.view(targets.size(0), -1)\n",
    "        \n",
    "        intersection = (preds_flat * targets_flat).sum(dim=1)\n",
    "        union = preds_flat.sum(dim=1) + targets_flat.sum(dim=1)\n",
    "        \n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()  # Average over batch\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, smooth=1e-6):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha  # Weight for Dice (1 - alpha for BCE)\n",
    "        self.dice = DiceLoss(smooth=smooth)\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        dice_loss = self.dice(preds, targets)\n",
    "        bce_loss = self.bce(preds, targets)\n",
    "        return self.alpha * dice_loss + (1 - self.alpha) * bce_loss\n",
    "\n",
    "def seg_data_collator(features):\n",
    "    pixel_values = torch.stack([f[\"pixel_values\"] for f in features])\n",
    "    labels = torch.stack([f[\"label\"] for f in features])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the transformations\n",
    "train_transform = A.Compose([\n",
    "    # 1. Geometric: Handles both image and mask\n",
    "    #A.HorizontalFlip(p=0.5),\n",
    "    #A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    \n",
    "    # 1. Photometric: Only affects the image\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.4),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    \n",
    "    # 2. Robustness: CoarseDropout to force feature learning\n",
    "    A.CoarseDropout(\n",
    "        num_holes_range=(2, 4), \n",
    "        hole_height_range=(10, 20), \n",
    "        hole_width_range=(10, 20), \n",
    "        #num_holes_range=(3, 6),\n",
    "        #hole_height_range=(10, 20),\n",
    "        #hole_width_range=(10, 20),\n",
    "        p=0.3),\n",
    "    \n",
    "    # 3. Normalization (using ImageNet stats)\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "\n",
    "def preprocess_fn(examples):\n",
    "    images = [np.array(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    # Ensure masks are single-channel (L) for segmentation\n",
    "    masks = [np.array(mask.convert(\"L\")) for mask in examples[\"label\"]]\n",
    "    \n",
    "    inputs = {\"pixel_values\": [], \"labels\": []}\n",
    "    \n",
    "    for img, mask in zip(images, masks):\n",
    "        # Apply Albumentations\n",
    "        augmented = train_transform(image=img, mask=mask)\n",
    "        \n",
    "        inputs[\"pixel_values\"].append(augmented[\"image\"])\n",
    "        # Ensure mask is long type and scaled (0 and 1)\n",
    "        inputs[\"labels\"].append(augmented[\"label\"].long())\n",
    "        \n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d387f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "    # Upsample logits to match label size\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(\n",
    "        predictions=predictions, \n",
    "        references=labels, \n",
    "        num_labels=2, \n",
    "        ignore_index=255\n",
    "    )\n",
    "\n",
    "class LogoSegmentationTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")           # shape (batch, h, w)\n",
    "        outputs = model(**inputs)               # Segformer outputs\n",
    "        logits = outputs.logits                 # (batch, 2, h, w)\n",
    "\n",
    "        # Option A: Keep CE (very stable baseline)\n",
    "        # loss_fct = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
    "        # loss = loss_fct(logits, labels.long())\n",
    "\n",
    "        # Option B: Dice + BCE (your preference)\n",
    "        preds = torch.softmax(logits, dim=1)[:, 1]   # prob of logo class\n",
    "        preds = preds.unsqueeze(1)                   # (B,1,H,W)\n",
    "\n",
    "        labels_binary = (labels == 1).float().unsqueeze(1)  # (B,1,H,W)\n",
    "\n",
    "        # Reuse your earlier losses\n",
    "        #dice_loss = DiceLoss()(preds, labels_binary)\n",
    "        #dice_loss = nn.BCELoss()(preds, labels_binary)\n",
    "        \n",
    "        #bce_loss  = F.binary_cross_entropy_with_logits(logits[:,1,:,:], labels_binary.squeeze(1))\n",
    "        # or F.binary_cross_entropy(preds, labels_binary)\n",
    "\n",
    "        #loss = 0.5 * dice_loss + 0.5 * bce_loss\n",
    "        loss = CombinedLoss()(preds, labels_binary)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
    "    num_labels=1,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\"\"\"\n",
    "class UNetPlusPlusConfig(PretrainedConfig):\n",
    "    model_type = \"unetplusplus\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder_name='resnet34', \n",
    "        encoder_depth=5, \n",
    "        encoder_weights='imagenet', \n",
    "        decoder_use_norm='batchnorm', \n",
    "        decoder_channels=(256, 128, 64, 32, 16), \n",
    "        decoder_attention_type=None, \n",
    "        decoder_interpolation='nearest', \n",
    "        in_channels=3, \n",
    "        classes=1, \n",
    "        activation=None, \n",
    "        aux_params=None\n",
    "        **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder_name = encoder_name\n",
    "        self.encoder_depth = encoder_depth\n",
    "        self.encoder_weights = encoder_weights\n",
    "        self.decoder_use_norm = decoder_use_norm\n",
    "        self.decoder_channels = decoder_channels\n",
    "        self.decoder_attention_type = decoder_attention_type\n",
    "        self.decoder_interpolation = decoder_interpolation\n",
    "        self.in_channels = in_channels\n",
    "        self.classes = classes\n",
    "        self.activation = activation\n",
    "        self.aux_params = aux_params\n",
    "\n",
    "class UNetPlusPlusHF(PreTrainedModel):\n",
    "    config_class = UNetPlusPlusConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = smp.UnetPlusPlus(\n",
    "            encoder_name=config.encoder_name,\n",
    "            encoder_depth=config., \n",
    "            encoder_weights=config.encoder_weights, \n",
    "            decoder_use_norm=config., \n",
    "            decoder_channels=config., \n",
    "            decoder_attention_type=config., \n",
    "            decoder_interpolation=config., \n",
    "            in_channels=config., \n",
    "            classes=config., \n",
    "            activation=config., \n",
    "            aux_params=config.\n",
    "        )\n",
    "    \"\"\"\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        logits = self.model(pixel_values)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # We reuse the DiceBCELoss logic from before\n",
    "            loss_fct = CombinedLoss()\n",
    "            # SMP outputs [Batch, Classes, H, W]\n",
    "            # We take the 'logo' channel (index 1) for binary comparison\n",
    "            loss = loss_fct(logits[:, 1, :, :], labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
    "    \"\"\"\n",
    "\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name='resnet34', \n",
    "    encoder_depth=5, \n",
    "    encoder_weights='imagenet', \n",
    "    decoder_use_norm='batchnorm', \n",
    "    decoder_channels=(256, 128, 64, 32, 16), \n",
    "    decoder_attention_type=None, \n",
    "    decoder_interpolation='nearest', \n",
    "    in_channels=3, \n",
    "    classes=1, \n",
    "    activation=None, \n",
    "    aux_params=None\n",
    "    )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output_image_segmentation/\",\n",
    "    learning_rate=6e-5,\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=8,    # adjust to your GPU (4â€“16 typical)\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,       # important for custom datasets\n",
    "    push_to_hub=False,                 # set True later if you want\n",
    "    report_to=\"none\",                  # or \"wandb\", \"tensorboard\"\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mean_iou\",\n",
    "    greater_is_better=True,\n",
    "    bf16=True,                         # if GPU supports\n",
    "    save_total_limit = 3,\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    do_predict = True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e30cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = f\"/home/yassir/projects/image_semantic_segmentation/data/processed/\"\n",
    "ds = load_from_disk(ds_path)\n",
    "\n",
    "# Apply to your dataset\n",
    "ds[\"train\"].set_transform(preprocess_fn)\n",
    "ds[\"validation\"].set_transform(preprocess_fn)\n",
    "ds[\"test\"].set_transform(preprocess_fn)\n",
    "\n",
    "\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name='resnet34', \n",
    "    encoder_depth=5, \n",
    "    encoder_weights='imagenet', \n",
    "    decoder_use_norm='batchnorm', \n",
    "    decoder_channels=(256, 128, 64, 32, 16), \n",
    "    decoder_attention_type=None, \n",
    "    decoder_interpolation='nearest', \n",
    "    in_channels=3, \n",
    "    classes=1, \n",
    "    activation=None, \n",
    "    aux_params=None,\n",
    "    )\n",
    "model.config = Namespace(**model.config)\n",
    "model.config.use_cache = True\n",
    "\n",
    "trainer = LogoSegmentationTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=seg_data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = f\"/home/yassir/projects/image_semantic_segmentation/data/processed/\"\n",
    "ds = load_from_disk(ds_path)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5bb2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_semantic_segmentation",
   "language": "python",
   "name": "image_semantic_segmentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
